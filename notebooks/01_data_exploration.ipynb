{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:2.5em; font-weight:bold; text-align:center; margin-top:20px;\">LSTM Financial Time Series Data Exploration</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook explores the financial time series datasets used for LSTM forecasting. We'll analyze stock prices, forex data, economic indicators, and their relationships to understand patterns and prepare the data for LSTM modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the sample datasets:\n",
    "- Stock price data (daily)\n",
    "- Forex data (hourly)\n",
    "- Economic indicators (monthly)\n",
    "- Processed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "stock_data = pd.read_csv('data/stock_price_sample.csv', parse_dates=['Date'], index_col='Date')\n",
    "forex_data = pd.read_csv('data/forex_sample.csv', parse_dates=['Timestamp'], index_col='Timestamp')\n",
    "econ_data = pd.read_csv('data/economic_indicators_sample.csv', parse_dates=['Date'], index_col='Date')\n",
    "processed_data = pd.read_csv('data/processed_features_sample.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Display basic information for each dataset\n",
    "print(\"=\"*50)\n",
    "print(\"Stock Price Data\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {stock_data.shape}\")\n",
    "print(f\"Time range: {stock_data.index.min()} to {stock_data.index.max()}\")\n",
    "print(f\"Columns: {stock_data.columns.tolist()}\")\n",
    "print(stock_data.head())\n",
    "print(\"\\nData types:\")\n",
    "print(stock_data.dtypes)\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(stock_data.describe())\n",
    "\n",
    "print(\"\\n\"+\"=\"*50)\n",
    "print(\"Forex Data\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {forex_data.shape}\")\n",
    "print(f\"Time range: {forex_data.index.min()} to {forex_data.index.max()}\")\n",
    "print(f\"Columns: {forex_data.columns.tolist()}\")\n",
    "print(forex_data.head())\n",
    "print(\"\\nData types:\")\n",
    "print(forex_data.dtypes)\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(forex_data.describe())\n",
    "\n",
    "print(\"\\n\"+\"=\"*50)\n",
    "print(\"Economic Indicators Data\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {econ_data.shape}\")\n",
    "print(f\"Time range: {econ_data.index.min()} to {econ_data.index.max()}\")\n",
    "print(f\"Columns: {econ_data.columns.tolist()}\")\n",
    "print(econ_data.head())\n",
    "print(\"\\nData types:\")\n",
    "print(econ_data.dtypes)\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(econ_data.describe())\n",
    "\n",
    "print(\"\\n\"+\"=\"*50)\n",
    "print(\"Processed Features Data\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Shape: {processed_data.shape}\")\n",
    "print(f\"Time range: {processed_data.index.min()} to {processed_data.index.max()}\")\n",
    "print(f\"Columns: {processed_data.columns.tolist()}\")\n",
    "print(processed_data.head())\n",
    "print(\"\\nData types:\")\n",
    "print(processed_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Missing Values and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for missing values in our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each dataset\n",
    "print(\"Missing values in Stock Price Data:\")\n",
    "print(stock_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in Forex Data:\")\n",
    "print(forex_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in Economic Indicators Data:\")\n",
    "print(econ_data.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in Processed Features Data:\")\n",
    "print(processed_data.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\nDuplicate rows in Stock Price Data:\", stock_data.index.duplicated().sum())\n",
    "print(\"Duplicate rows in Forex Data:\", forex_data.index.duplicated().sum())\n",
    "print(\"Duplicate rows in Economic Indicators Data:\", econ_data.index.duplicated().sum())\n",
    "print(\"Duplicate rows in Processed Features Data:\", processed_data.index.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Time Series Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the time series data to identify patterns, trends, and seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to plot time series data\n",
    "def plot_time_series(data, title, columns=None):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    if columns:\n",
    "        data = data[columns]\n",
    "    \n",
    "    data.plot()\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot stock price data\n",
    "plot_time_series(stock_data, 'Stock Price Data (January 2023)', ['Close'])\n",
    "\n",
    "# Plot High, Low, Open, Close in one chart\n",
    "plot_time_series(stock_data, 'OHLC Stock Price Data (January 2023)', ['Open', 'High', 'Low', 'Close'])\n",
    "\n",
    "# Plot trading volume\n",
    "plt.figure(figsize=(15, 5))\n",
    "stock_data['Volume'].plot(kind='bar', color='skyblue')\n",
    "plt.title('Trading Volume (January 2023)', fontsize=15)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot forex data\n",
    "plot_time_series(forex_data, 'Forex Data (January 3, 2023)', ['Close'])\n",
    "\n",
    "# Plot economic indicators\n",
    "plot_time_series(econ_data, 'Economic Indicators (2022-2023)')\n",
    "\n",
    "# Plot selected technical indicators from processed data\n",
    "plot_time_series(processed_data, 'Technical Indicators (January 2023)', \n",
    "                ['SMA_20', 'EMA_12', 'RSI_14'])\n",
    "\n",
    "# Plot MACD indicators\n",
    "plot_time_series(processed_data, 'MACD Indicators (January 2023)', \n",
    "                ['MACD', 'MACD_Signal', 'MACD_Hist'])\n",
    "\n",
    "# Plot Bollinger Bands with Close price\n",
    "plt.figure(figsize=(15, 8))\n",
    "processed_data[['Close', 'BB_Upper', 'BB_Lower']].plot()\n",
    "plt.title('Bollinger Bands (January 2023)', fontsize=15)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform statistical analysis on the time series data to understand stationarity, autocorrelation, and other properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stationarity(time_series, title):\n",
    "    \"\"\"Check stationarity of a time series using ADF and KPSS tests\"\"\"\n",
    "    # Augmented Dickey-Fuller test\n",
    "    result_adf = adfuller(time_series.dropna())\n",
    "    print(f'ADF Statistic for {title}: {result_adf[0]:.4f}')\n",
    "    print(f'p-value: {result_adf[1]:.4f}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result_adf[4].items():\n",
    "        print(f'\\t{key}: {value:.4f}')\n",
    "    \n",
    "    # KPSS test\n",
    "    result_kpss = kpss(time_series.dropna())\n",
    "    print(f'\\nKPSS Statistic for {title}: {result_kpss[0]:.4f}')\n",
    "    print(f'p-value: {result_kpss[1]:.4f}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result_kpss[3].items():\n",
    "        print(f'\\t{key}: {value:.4f}')\n",
    "    \n",
    "    # Plot ACF and PACF\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    plot_acf(time_series.dropna(), ax=ax1, title=f'Autocorrelation Function for {title}')\n",
    "    plot_pacf(time_series.dropna(), ax=ax2, title=f'Partial Autocorrelation Function for {title}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Check stationarity of closing prices\n",
    "check_stationarity(stock_data['Close'], 'Stock Closing Prices')\n",
    "\n",
    "# Check stationarity of returns\n",
    "if 'Returns' in processed_data.columns:\n",
    "    check_stationarity(processed_data['Returns'], 'Stock Returns')\n",
    "\n",
    "# Check stationarity of forex closing prices\n",
    "check_stationarity(forex_data['Close'], 'Forex Closing Prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the correlations between different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for stock data\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(stock_data.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix - Stock Price Data', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis for processed features\n",
    "# Select a subset of columns to keep the plot readable\n",
    "selected_features = ['Open', 'Close', 'Volume', 'SMA_20', 'EMA_12', 'RSI_14', \n",
    "                     'MACD', 'MACD_Signal', 'ATR_14', 'Returns', 'Volatility_20']\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(processed_data[selected_features].corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix - Selected Technical Indicators', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation between economic indicators\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(econ_data.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix - Economic Indicators', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the distributions of key variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot distributions\n",
    "def plot_distributions(data, columns, title):\n",
    "    n_cols = len(columns)\n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=(15, 5))\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        sns.histplot(data[col], kde=True, ax=axes[i] if n_cols > 1 else axes)\n",
    "        if n_cols > 1:\n",
    "            axes[i].set_title(col)\n",
    "        else:\n",
    "            axes.set_title(col)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Distribution of stock prices\n",
    "plot_distributions(stock_data, ['Close'], 'Distribution of Stock Closing Prices')\n",
    "\n",
    "# Distribution of trading volume\n",
    "plot_distributions(stock_data, ['Volume'], 'Distribution of Trading Volume')\n",
    "\n",
    "# Distribution of returns and volatility\n",
    "if 'Returns' in processed_data.columns and 'Volatility_20' in processed_data.columns:\n",
    "    plot_distributions(processed_data, ['Returns', 'Volatility_20'], \n",
    "                      'Distribution of Returns and Volatility')\n",
    "\n",
    "# Distribution of technical indicators\n",
    "plot_distributions(processed_data, ['RSI_14', 'ATR_14'], \n",
    "                  'Distribution of Technical Indicators')\n",
    "\n",
    "# QQ plots for returns (if available)\n",
    "if 'Log_Returns' in processed_data.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    from scipy import stats\n",
    "    stats.probplot(processed_data['Log_Returns'].dropna(), dist=\"norm\", plot=plt)\n",
    "    plt.title('Q-Q Plot of Log Returns', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Engineering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the relationship between raw prices and engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relationship between closing price and moving averages\n",
    "plt.figure(figsize=(15, 8))\n",
    "processed_data[['Close', 'SMA_20', 'EMA_12']].plot()\n",
    "plt.title('Closing Price vs Moving Averages', fontsize=15)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot RSI and closing price\n",
    "fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Closing Price', color=color)\n",
    "ax1.plot(processed_data.index, processed_data['Close'], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('RSI', color=color)\n",
    "ax2.plot(processed_data.index, processed_data['RSI_14'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.axhline(y=70, color='gray', linestyle='--')\n",
    "ax2.axhline(y=30, color='gray', linestyle='--')\n",
    "\n",
    "plt.title('Closing Price vs RSI', fontsize=15)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot returns vs volatility\n",
    "if 'Returns' in processed_data.columns and 'Volatility_20' in processed_data.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(processed_data['Returns'], processed_data['Volatility_20'])\n",
    "    plt.xlabel('Returns')\n",
    "    plt.ylabel('Volatility (20-day)')\n",
    "    plt.title('Returns vs Volatility', fontsize=15)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Time Series Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decompose the time series to identify trend, seasonality, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required library\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Time series decomposition of stock closing prices\n",
    "try:\n",
    "    # Note: For a proper decomposition, we'd need more data points\n",
    "    # This is for illustration purposes with the sample data\n",
    "    decomposition = seasonal_decompose(stock_data['Close'], model='additive', period=5)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(411)\n",
    "    plt.plot(decomposition.observed)\n",
    "    plt.title('Observed', fontsize=12)\n",
    "    plt.subplot(412)\n",
    "    plt.plot(decomposition.trend)\n",
    "    plt.title('Trend', fontsize=12)\n",
    "    plt.subplot(413)\n",
    "    plt.plot(decomposition.seasonal)\n",
    "    plt.title('Seasonality', fontsize=12)\n",
    "    plt.subplot(414)\n",
    "    plt.plot(decomposition.resid)\n",
    "    plt.title('Residuals', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except:\n",
    "    print(\"Not enough data points for seasonal decomposition with the sample data.\")\n",
    "    print(\"This would work with a larger dataset spanning several periods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Volatility Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze volatility clustering in returns, which is important for financial time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility clustering in returns\n",
    "if 'Returns' in processed_data.columns:\n",
    "    # Plot returns\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(211)\n",
    "    plt.plot(processed_data['Returns'])\n",
    "    plt.title('Stock Returns', fontsize=15)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot squared returns (proxy for volatility)\n",
    "    plt.subplot(212)\n",
    "    plt.plot(processed_data['Returns']**2)\n",
    "    plt.title('Squared Returns (Volatility Proxy)', fontsize=15)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Autocorrelation of squared returns\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_acf(processed_data['Returns'].dropna()**2, lags=20)\n",
    "    plt.title('Autocorrelation of Squared Returns', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Data Preparation for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's prepare the data for LSTM modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary utilities\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"Create sequences for LSTM input\"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "# Example: Prepare closing price data for LSTM\n",
    "# 1. Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(stock_data[['Close']])\n",
    "\n",
    "# 2. Create sequences\n",
    "seq_length = 5  # Use 5 days of data to predict the next day\n",
    "X, y = create_sequences(scaled_data, seq_length)\n",
    "\n",
    "# 3. Split into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Training data shape:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(\"\\nValidation data shape:\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")\n",
    "print(\"\\nTest data shape:\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "# Visualize a sample sequence\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(scaler.inverse_transform(X_train[0]), label='Input Sequence')\n",
    "plt.scatter(seq_length, scaler.inverse_transform(y_train[0]), color='r', label='Target')\n",
    "plt.title(f'Sample LSTM Input Sequence and Target (Sequence Length: {seq_length})', fontsize=15)\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Normalized Closing Price')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exploratory data analysis, we examined several financial time series datasets:\n",
    "1. **Stock Price Data**: Daily OHLCV data showing typical stock market patterns\n",
    "2. **Forex Data**: Hourly exchange rate data showing smaller price movements\n",
    "3. **Economic Indicators**: Monthly macroeconomic indicators with strong correlations\n",
    "4. **Processed Features**: Technical indicators derived from price data\n",
    "\n",
    "Key findings:\n",
    "- The stock price data shows volatility and trends typical of financial markets\n",
    "- Technical indicators like RSI, MACD, and Bollinger Bands provide useful signals\n",
    "- The returns data shows volatility clustering, a common feature in financial time series\n",
    "- The economic indicators have interesting correlations with each other\n",
    "\n",
    "Next steps for LSTM modeling:\n",
    "1. Extend the data preparation to include multivariate features\n",
    "2. Implement the LSTM models with different architectures\n",
    "3. Evaluate model performance with appropriate metrics\n",
    "4. Compare LSTM predictions with traditional forecasting methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.21 (docker_for_data_science_projects)",
   "language": "python",
   "name": "docker_for_data_science_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
